{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Wrangling with Breeds on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Understand ways to find a data set and to prepare a data set for machine learning and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities \n",
    "**In this section of the training you will**\n",
    "- Transfer a data set from the shared location on the server to your current directory. \n",
    "- View your initial data\n",
    "- Clean and normalize the data set\n",
    "- Augment the data\n",
    "- Organize the data into training and testing groups \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a Dataset\n",
    "\n",
    "### Research Existing Data Sets\n",
    "\n",
    "Artificial intelligence projects depend upon data. When beginning a project, data scientists look for existing data sets that are similar to or match the given problem. This saves time and money, and leverages the work of others, building upon the body of knowledge for all future projects. \n",
    "\n",
    "Typically you begin with a search engine query. For this project, we were looking for a data set with an unencumbered license.  \n",
    "\n",
    "This project starts with the Oxford IIIT Pet Data set http://www.robots.ox.ac.uk/~vgg/data/pets/ , a 37-category pet data set with roughly 200 images for each class. The images have a large variations in scale, pose, and lighting. All images have an associated ground truth annotation of breed, head region of interest (ROI), and pixel-level trimap segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\"The pet images were downloaded from Catster* and Dogster*, two social web sites dedicated to the collection and discussion of images of pets, from Flickr* groups, and from Google Images*. People uploading images to Catster and Dogster provide the breed information as well, and the Flickr groups are specific to each breed, which simplifies tagging. For each of the 37 breeds, about 2,000 – 2,500 images were downloaded from these data sources to form a pool of candidates for inclusion in the dataset. From this candidate list, images were dropped if any of the following conditions applied, as judged by the annotators: (i) the image was gray scale, (ii) another image portraying the same animal existed (which happens frequently in Flickr), (iii) the illumination was poor, (iv) the pet was not centered in the image, or (v) the pet was wearing clothes. The most common problem in all the data sources, however, was found to be errors in the breed labels. Thus labels were reviewed by the human annotators and fixed whenever possible. When fixing was not possible, for instance because the pet was a cross breed, the image was dropped.”\n",
    "\n",
    "From *Cats and Dogs*, http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Your Data\n",
    "![Fetch Data](assets/part1_1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Click the cell below and then click **Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf breeds/\n",
    "!mkdir -p breeds\n",
    "!rsync -r --progress /data/aidata/breeds/original/ breeds/\n",
    "\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# View the Baseline Data\n",
    "\n",
    "Take a look at the images in your data set. This will give you some idea as to how much cleaning and normalizing will be required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![View and Understand Your Data](assets/part1_2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "In the cell below, update the display_images function by changing the **numOfImages** parameter to a number from 1 to 5. Click **Save**, and then click **Run**.\n",
    " \n",
    "*Hint: The display_images function sets a display grid showing NxN pet images. The default number of images is set to **?**. Change the **?** character to something greater than 1; for example, **numOfImages = 6**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "def get_category(file):\n",
    "    m = re.search(\"\\d\", file, re.IGNORECASE)\n",
    "    if m:\n",
    "        return file[:m.start() - 1].lower().split(\"/\")[1]\n",
    "\n",
    "def display_images(file_names, numOfImages = ?):\n",
    "    indicies = random.sample(range(len(file_names)), numOfImages * numOfImages)\n",
    "    train_images = [file_names[i] for i in indicies]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=numOfImages,ncols=numOfImages, figsize=(15,15), sharex=True, sharey=True, frameon=False)\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        curr_i = train_images[i]\n",
    "        imgplot = mpimg.imread(curr_i)\n",
    "        ax.imshow(imgplot)\n",
    "        ax.text(10,20,get_category(curr_i), fontdict={\"backgroundcolor\": \"black\",\"color\": \"white\" })\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)    \n",
    "    \n",
    "    \n",
    "display_images(glob.glob('breeds/*.jpg'))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Clean and Normalize the Data\n",
    "Existing image recognition data-sets often include images of multiple dimensions, color mixed with black and white photos, maybe even line art plus photos. File names may follow multiple formats, and the subject matter within the images may be single, multiple, profile, straight-on face, back of head, surrounded by a complex background or more. \n",
    "Cleaning and normalizing the data means fixing the inconsistencies so that the machine processing can occur with minimal errors. Oftentimes data cleaning is tedious and requires significant time commitment. \n",
    "Data preprocessing techniques include:\n",
    "1.\tData cleaning − Eliminates noise and resolves inconsistencies in the data. \n",
    "2.\tData integration − Migrates data from various different sources into one coherent source, such as a data warehouse.\n",
    "3.\tData transformation – Standardizes or normalizes any form of data.\n",
    "4.\tData reduction – Reduces the size of the data by aggregating it.\n",
    "\n",
    "Another name for this effort is extract, transform, and load (ETL).\n",
    "This project required the team to normalize the file dimensions, file names and create a data layout expected by the framework. \n",
    "\n",
    "It is common for the data cleanup tasks to be pared with framework and topology selection because different topologies expect different data layouts and formats. When experimenting with different topologies it might be necessary to have several copies of the data in various formats.  Multiple copies of data-sets can take up a lot of space, so ensure you’ve got lots of storage and processing capability.\n",
    "\n",
    "![Clean and Normalize the Data](assets/part1_3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "The code in the next cell performs some of the cleanup tasks. Review the code and notice that it is removing corrupt files, files with the wrong format, and files with incorrect metadata.\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "for file in glob.glob(\"breeds/*\"):\n",
    "    if not file.endswith(\".jpg\"):\n",
    "        #Not ending in .jpg\n",
    "        print(\"Deleting (.mat): \" + file)\n",
    "        os.remove(os.path.join(os.getcwd(), file))\n",
    "    else: \n",
    "        flags = cv2.IMREAD_COLOR\n",
    "        im = cv2.imread(file, flags)\n",
    "        \n",
    "        if im is None:\n",
    "            #Can't read in image\n",
    "            print(\"Deleting (None): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "        elif len(im.shape) != 3:\n",
    "            #Wrong amount of channels\n",
    "            print(\"Deleting (len != 3): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "        elif im.shape[2] != 3:\n",
    "            #Wrong amount of channels\n",
    "            print(\"Deleting (shape[2] != 3): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "            \n",
    "        with open(os.path.join(os.getcwd(), file), 'rb') as f:\n",
    "            check_chars = f.read()\n",
    "        if check_chars[-2:] != b'\\xff\\xd9':\n",
    "            #Wrong ending metadata for jpg standard\n",
    "            print('Deleting (xd9): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif check_chars[:4] != b'\\xff\\xd8\\xff\\xe0':\n",
    "            #Wrong Start Marker / JFIF Marker metadata for jpg standard\n",
    "            print('Deleting (xd8/xe0): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif check_chars[6:10] != b'JFIF':\n",
    "            #Wrong Identifier metadata for jpg standard\n",
    "            print('Deleting (xd8/xe0): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif \"beagle_116.jpg\" in file or \"chihuahua_121.jpg\" in file:\n",
    "            #Using EXIF Data to determine this\n",
    "            print('Deleting (corrupt jpeg data): ', file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))     \n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Your Data\n",
    "\n",
    "Most of the time you’re cleaning data and removing noise. Since our app needs to work with images of wet, muddy, or injured animals, or perhaps blurry images because the animal is running away in fear, we actually need to ADD noise to the data-set. \n",
    "\n",
    "We decided to add image noise by building a small program to flip, flop, blur, and extract color channels from the images in the dataset. These actions expanded our training data-set by 6x.\n",
    "\n",
    "The cell below uses a parallel method to scale the image processing tasks to all available processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Augment Your Data](assets/part1_4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#echo \"Start resizing to 227x227\"\n",
    "#parallel -j 200 convert {} -resize 227x227 -filter spline -unsharp 0x6+0.5+0 -background black -gravity center -extent 227x227  {} ::: *.jpg\n",
    "#echo \"Resizing done\"\n",
    "\n",
    "#mkdir flop\n",
    "#echo \"Start augmentation 1\"\n",
    "#parallel -j 200 convert {} -flop flop/{.}-flop.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 1\"\n",
    "\n",
    "#mkdir flip\n",
    "#echo \"Start augmentation 2\"\n",
    "#parallel -j 200 convert {} -transverse -rotate 90 flip/{.}-flip.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 2\"\n",
    "\n",
    "#mkdir blur\n",
    "#echo \"Start augmentation 3\"\n",
    "#parallel -j 200 convert {} -blur 0x1 blur/{.}-blur.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 3\"\n",
    "\n",
    "#mkdir red\n",
    "#echo \"Start augmentation 4\"\n",
    "#parallel -j 200 convert {} -channel R -separate red/{.}-red.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 4\"\n",
    "\n",
    "#mkdir blue\n",
    "#echo \"Start augmentation 5\"\n",
    "#parallel -j 200 convert {} -channel B -separate blue/{.}-blue.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 5\"\n",
    "\n",
    "#mkdir green\n",
    "#echo \"Start augmentation 6\"\n",
    "#parallel -j 200 convert {} -channel G -separate green/{.}-green.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 6\"\n",
    "\n",
    "#echo \"Copying augmented data to main folder\"\n",
    "#cp flop/* flip/* blur/* red/* blue/* green/* .\n",
    "\n",
    "#echo \"Augmentation done\"\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "def resize_image(file, size=227):\n",
    "    black_background = Image.new('RGB', (size, size), \"black\")\n",
    "    img = Image.open(file)\n",
    "    img.thumbnail((size,size))\n",
    "    x, y = img.size\n",
    "    black_background.paste(img, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    black_background.save(file)\n",
    "    return black_background\n",
    "  \n",
    "pool = Pool()\n",
    "for i, _ in enumerate(pool.map(resize_image, glob.glob(\"breeds/*\"))):\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r{0} out of {1} processed'.format(i+1, len(glob.glob(\"breeds/*\"))))\n",
    "\n",
    "sys.stdout.write('\\n')\n",
    "sys.stdout.flush()      \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# View Results \n",
    "\n",
    "The script you just ran flipped and flopped the images, added blur, and extracted each color channel. Take a look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![View Results](assets/part1_5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(glob.glob('breeds/*'))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Organize Data for Consumption by Caffe*\n",
    "\n",
    "The framework you choose for your project determines how you need to organize your data. After extensive experimentation, we selected Caffe for this project. This section describes how to organize your data layers.\n",
    "\n",
    "Our data needs to be organized in a specific manner. Caffe requires separate data sets for training and validation and that the data be stored in two separate folders. Why separate images sets for training and validation? To prevent “overfitting” which occurs when you train and test on the same images. You train on a set, then test on a new/different set to validate that the machine is truly learning to recognize the images. \n",
    "\n",
    "Our folders are named **train** and **val**. We used the industry standard ratio of 80% train and 20% test/validation to split the data set.\n",
    "\n",
    "Within the test/val folders we create category folders with every breed in the dataset, within which are stored the images of the cat breeds and dog breeds.  This next cell of code creates the data layout as expected by Caffe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Organize Data for Consumption by Framework](assets/part1_6.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "In the cell below, set the **train_ratio** to **0.8** and then click **Run**.\n",
    "\n",
    "*Hint: We need to set the train_ratio = ? to a value between 0 and 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import errno\n",
    "import math\n",
    "\n",
    "def get_category(file):\n",
    "    m = re.search(\"\\d\", file, re.IGNORECASE)\n",
    "    if m:\n",
    "        return file[:m.start() - 1].lower()\n",
    "\n",
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "file_names = os.listdir('breeds')               \n",
    "category_names = [ get_category(file) for file in file_names]\n",
    "category_names = [ name for name in category_names if name is not None ]\n",
    "category_names = sorted(list(set(category_names)))\n",
    "for category in category_names:\n",
    "    make_sure_path_exists(\"breeds/train/\" + str(category))\n",
    "    make_sure_path_exists(\"breeds/val/\" + str(category))\n",
    "\n",
    "train_ratio = ?\n",
    "train_txt = {}\n",
    "val_txt = {}\n",
    "\n",
    "for idx, category in enumerate(category_names):\n",
    "    category_list = []\n",
    "    for file in file_names:\n",
    "        if category.lower() in file.lower():\n",
    "            category_list.append(file)\n",
    "    \n",
    "    category_list = sorted(category_list)\n",
    "    split_ratio = math.floor(len(category_list) * train_ratio)\n",
    "    train_list = category_list[:split_ratio]\n",
    "    test_list = category_list[split_ratio:]\n",
    "    \n",
    "    for i, file in enumerate(train_list):\n",
    "        os.rename(\"breeds/\" + file, \"breeds/train/\" + str(category) + \"/\" + file)\n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r>> Moving train image %d to category folder %s' % (i+1, category))\n",
    "            sys.stdout.flush()\n",
    "        train_txt[str(category) + \"/\" + file] = idx\n",
    "        \n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()        \n",
    "        \n",
    "    for i, file in enumerate(test_list):\n",
    "        os.rename(\"breeds/\" + file, \"breeds/val/\" + str(category) + \"/\" + file)\n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r>> Moving validation image %d to category folder %s' % (i+1, category))\n",
    "            sys.stdout.flush()\n",
    "        val_txt[str(category) + \"/\" + file] = idx\n",
    "        \n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()        \n",
    "        \n",
    "print(\"Done splitting data\")        \n",
    "        \n",
    "train = open(\"breeds/train.txt\", \"w\")\n",
    "for key, val in train_txt.items():\n",
    "    train.write(\"{0} {1}\\n\".format(key, val))\n",
    "train.close()\n",
    "print(\"Wrote train.txt\")\n",
    "\n",
    "validation = open(\"breeds/val.txt\", \"w\")\n",
    "for key, val in val_txt.items():\n",
    "    validation.write(\"{0} {1}\\n\".format(key, val))\n",
    "validation.close()\n",
    "print(\"Wrote val.txt\")\n",
    "\n",
    "categories = open(\"breeds/categories.txt\", \"w\")\n",
    "for val in category_names:\n",
    "    categories.write(\"{0}\\n\".format(val))\n",
    "categories.close()\n",
    "print(\"Wrote categories.txt\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Confirm Folder Structure is Correct\n",
    "\n",
    "Notice we have folders for each breed category within our Train and Validation folders. The images of each breed have been sorted into their respective folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confirm Folder Structure is Correct](assets/part1_7.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"breeds\"):\n",
    "    level = root.replace(os.getcwd(), '').count(os.sep)\n",
    "    print('{0}{1}/'.format('    ' * level, os.path.basename(root)))\n",
    "    for file in files[:5]:\n",
    "        print('{0}{1}'.format('    ' * (level + 1), file))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Data for Ingestion\n",
    "\n",
    "### Data Layers\n",
    "Data enters Caffe through data layers. Data can come from efficient databases (LevelDB or LMDB), directly from memory, or, when efficiency is not critical, from files on disk in HDF5 or common image formats.\n",
    "\n",
    "### Transform data set to an LMDB dataset and determine the mean for all images\n",
    "So far we have a set of folders and a lot of image files. To optimize data load times, we create an LMDB database out of our many image files. The database will create pointers to the image files and identify them by number so that image processing and training can occur more quickly and efficiently. \n",
    "\n",
    "When we create a mean value for all images, it is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. This step gives us three values; one for each color channel Red, Green, and Blue. It is a value in between 0 to 255. Pixel values outside of this range are invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimize Data for Ingestion](assets/part1_8.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!echo \"Creating train lmdb...\"\n",
    "\n",
    "!rm -rf breeds/train_lmdb\n",
    "\n",
    "!/glob/intel-python/versions/2018u2/intelpython3/bin/convert_imageset \\\n",
    "    --shuffle \\\n",
    "    breeds/train/ \\\n",
    "    breeds/train.txt \\\n",
    "    breeds/train_lmdb\n",
    "\n",
    "!echo \"Creating val lmdb...\"\n",
    "\n",
    "!rm -rf breeds/val_lmdb\n",
    "\n",
    "!/glob/intel-python/versions/2018u2/intelpython3/bin/convert_imageset \\\n",
    "    --shuffle \\\n",
    "    breeds/val/ \\\n",
    "    breeds/val.txt \\\n",
    "    breeds/val_lmdb\n",
    "\n",
    "!echo \"Creating train_mean.binaryproto...\"\n",
    "\n",
    "!/glob/intel-python/versions/2018u2/intelpython3/bin/compute_image_mean breeds/train_lmdb \\\n",
    "  breeds/train_mean.binaryproto\n",
    "  \n",
    "!echo \"Creating val_mean.binaryproto...\"\n",
    "\n",
    "!/glob/intel-python/versions/2018u2/intelpython3/bin/compute_image_mean breeds/val_lmdb \\\n",
    "  breeds/val_mean.binaryproto\n",
    "\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### After all of this data wrangling we can actually begin the training process\n",
    "\n",
    "When we started this project, we always had an edge device in mind as our ultimate deployment platform. To that end we always considered three things when selecting our topology or network; time to train, size, and inference speed. \n",
    "\n",
    "**Time to Train:** Depending on the number of layers and computation required, a network can take a significantly shorter or longer time to train. Computation time and programmer time are costly resources, so wanted short training times.  \n",
    "\n",
    "**Size:** Since we're targeting an edge device and an Intel® Movidius™ Neural Compute Stick we must consider the size of the network that is allowed in memory as well as supported networks.\n",
    "\n",
    "**Inference speed:** Typically the deeper and larger the network, the slower the inference speed. In our use case we are working with a live video stream; we want at least 10 frames per second on inference.\n",
    "\n",
    "At this point we're going to continue with the Caffe framework plus the GoogeLeNet topology/network since we're currently working on a complex dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![googlenet](assets/googlenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training Breeds with Caffe and GoogleNet on CPU\n",
    "\n",
    "# Objective \n",
    "Understand the stages of preparing a data set for training using the Caffe framework and GoogLeNet topology. You will initiate training and view a completed graph.\n",
    "\n",
    "# Activities \n",
    "**In this section of the training you will**\n",
    "- View Solver and Train Prototxt\n",
    "- Start Training\n",
    "- Accuracy and Loss for Full Run\n",
    "- Looking at a Sample Image\n",
    "- Inference on a Sample Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver Files\n",
    "The solver.prototxt file is a configuration file used to tell Caffe how you want the network trained. It includes parameters that you can change to adjust the training.  Inside the solver file for example, you can set parameters to save checkpoints after NN iterations so that you can have a reference point. You can also tell Caffe to test your data after a given number of iterations as well. In fact, you can easily switch between CPU and GPU training by changing a single parameter.\n",
    "\n",
    "The train.prototxt file is a text description of the network you see in the image above. This is how Caffe ingests the network and knows how to pass the image data back and forth through the network during training.\n",
    "\n",
    "\n",
    "[Wiki for Solver parameters](https://github.com/BVLC/caffe/wiki/Solver-Prototxt)\n",
    "\n",
    "\n",
    "# Display Tunable Parameters for Training\n",
    "\n",
    "![Display Tunable Parameters for Training](assets/part2_1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo \"Displaying Solver Prototxt: \"\n",
    "!echo \"\"\n",
    "!cat breeds_googlenet_solver.prototxt\n",
    "!echo \"\"\n",
    "!echo \"Displaying Train Prototxt: \"\n",
    "!echo \"\"\n",
    "!cat breeds_googlenet_train.prototxt\n",
    "!echo \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training\n",
    "\n",
    "Let’s start training with Caffe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Start Training](assets/part2_2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Point Caffe to look at the correct solver file by setting the prototext section to **breeds_googlenet_solver.prototxt** and then click **Run**.\n",
    "\n",
    "\n",
    "*Hint: When we start training we have to point caffe at the correct solver file.  Fill in the **???**.prototxt section to **breeds_googlenet_solver.prototxt**. Then come back and run the next command.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ..\n",
    "!mkdir breeds_googlenet\n",
    "!/glob/intel-python/versions/2018u2/intelpython3/bin/caffe train -solver ??????.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## Accuracy and Loss for Breeds using GoogleNet Topology\n",
    "\n",
    "This is a graph of the completed training of Breeds using GoogLeNet. The image represents a 6-hour training session. If we had enough time to run through all of the training you would see something similar to the graphs below.\n",
    "\n",
    "![After 3 hours Graphs will look like below:](assets/part2_3.JPG)\n",
    "\n",
    "<img src=\"assets/GoogleNet_Breeds.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Look at a Sample Image\n",
    "\n",
    "We're going to use this image to run through the network and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Look at Sample Image](assets/part2_4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open('breeds/train/maine_coon/Maine_Coon_100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on an Image\n",
    "\n",
    "We can use the newly created frozen graph file to test a sample image.  We're using the label_image script that takes an image, frozen graph, labels.txt files, and displays the top five probabilities for the given image.\n",
    "\n",
    "![Inference on an Image](assets/part2_5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "files = glob.glob('breeds_googlenet/breeds_googlenet_iter_*.caffemodel')\n",
    "latest = max(files, key=os.path.getctime)\n",
    "\n",
    "caffe.set_mode_cpu()\n",
    "net = caffe.Net('deploy.prototxt', latest, caffe.TEST)\n",
    "\n",
    "net.blobs['data'].reshape(1,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          224, 224)  # image size is 227x227\n",
    "\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "with open('breeds/train_mean.binaryproto', 'rb') as f:\n",
    "    blob.ParseFromString(f.read())\n",
    "    data = np.array(blob.data).reshape([blob.channels, blob.height, blob.width])\n",
    "    mu = np.array([np.mean(data[0]), np.mean(data[1]), np.mean(data[2])])\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "\n",
    "#load image\n",
    "image = Image.open('breeds/train/maine_coon/Maine_Coon_100.jpg')\n",
    "data = np.asarray(image)\n",
    "transformed_image = transformer.preprocess('data', data)\n",
    "\n",
    "# copy the image data into the memory allocated for the net\n",
    "net.blobs['data'].data[...] = transformed_image\n",
    "\n",
    "# load categories.txt labels\n",
    "labels_file = 'breeds/categories.txt'\n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\n')\n",
    "\n",
    "### perform classification\n",
    "net.forward()\n",
    "\n",
    "# obtain the output probabilities\n",
    "output_prob = net.blobs['prob'].data[0]\n",
    "\n",
    "# sort top predictions from softmax output\n",
    "top_inds = output_prob.argsort()[::-1]\n",
    "\n",
    "print('Probabilities and Labels:', list(zip(output_prob[top_inds], labels[top_inds])))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting your dataset\n",
    "- Sorting your dataset\n",
    "- Generating LMDB Record\n",
    "- Training your dataset\n",
    "- Using your caffemodel to test image classification\n",
    "\n",
    "![Summary](assets/part2_6.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "What is Intel® Optimized Caffe, https://software.intel.com/en-us/videos/what-is-intel-optimized-caffe\n",
    "\n",
    "Caffe | Deep Learning Framework, http://caffe.berkeleyvision.org/\n",
    "\n",
    "Intel Caffe on GitHub*, https://github.com/intel/caffe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manufacturing Package Fault Detection Using Deep Learning, https://software.intel.com/en-us/articles/manufacturing-package-fault-detection-using-deep-learning\n",
    "\n",
    "Automatic Defect Inspection Using Deep Learning for Solar Farm, https://software.intel.com/en-us/articles/automatic-defect-inspection-using-deep-learning-for-solar-farm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notices**\n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\n",
    "\n",
    "Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.\n",
    "\n",
    "This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest forecast, schedule, specifications and roadmaps.\n",
    "\n",
    "The products and services described may contain defects or errors known as errata which may cause deviations from published specifications. Current characterized errata are available on request.\n",
    "\n",
    "Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting www.intel.com/design/literature.htm.\n",
    "\n",
    "This sample source code is released under the Intel Sample Source Code License Agreement.\n",
    "\n",
    "Intel, the Intel logo, Intel Xeon Phi, Movidius, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. \n",
    "\n",
    "*Other names and brands may be claimed as the property of others.\n",
    "\n",
    "© 2018 Intel Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
