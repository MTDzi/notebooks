{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Tensorflow Version to 1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://anaconda.org/intel/tensorflow/1.6.0/download/tensorflow-1.6.0-cp36-cp36m-linux_x86_64.whl --user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Wrangling with Breeds on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Understand ways to find a data set and to prepare a data set for machine learning and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities \n",
    "**In this section of the training you will**\n",
    "- Transfer a data set from the shared location on the server to your current directory. \n",
    "- View your initial data\n",
    "- Clean and normalize the data set\n",
    "- Organize the data into training and testing groups \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a Data set\n",
    "\n",
    "### Research Existing Data Sets\n",
    "\n",
    "Artificial intelligence projects depend upon data. When beginning a project, data scientists look for existing data sets that are similar to or match the given problem. This saves time and money, and leverages the work of others, building upon the body of knowledge for all future projects. \n",
    "\n",
    "Typically you begin with a search engine query. For this project, we were looking for a data set with an unencumbered license.  \n",
    "\n",
    "This project starts with the Oxford IIIT Pet Data set http://www.robots.ox.ac.uk/~vgg/data/pets/ , a 37-category pet data set with roughly 200 images for each class. The images have a large variations in scale, pose, and lighting. All images have an associated ground truth annotation of breed, head region of interest (ROI), and pixel-level trimap segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\"The pet images were downloaded from Catster* and Dogster*, two social web sites dedicated to the collection and discussion of images of pets, from Flickr* groups, and from Google Images*. People uploading images to Catster and Dogster provide the breed information as well, and the Flickr groups are specific to each breed, which simplifies tagging. For each of the 37 breeds, about 2,000 – 2,500 images were downloaded from these data sources to form a pool of candidates for inclusion in the dataset. From this candidate list, images were dropped if any of the following conditions applied, as judged by the annotators: (i) the image was gray scale, (ii) another image portraying the same animal existed (which happens frequently in Flickr), (iii) the illumination was poor, (iv) the pet was not centered in the image, or (v) the pet was wearing clothes. The most common problem in all the data sources, however, was found to be errors in the breed labels. Thus labels were reviewed by the human annotators and fixed whenever possible. When fixing was not possible, for instance because the pet was a cross breed, the image was dropped.”\n",
    "\n",
    "From *Cats and Dogs*, http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Your Data\n",
    "![Fetch Data](assets/part1_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity \n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf breeds/\n",
    "!mkdir -p breeds\n",
    "!rsync -r --progress /data/aidata/breeds/original/ breeds/\n",
    "\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# View the Baseline Data\n",
    "\n",
    "Take a look at the images in your data set. This gives you some idea as to how much cleaning and normalizing will be required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![View and Understand Your Data](assets/part1_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "In the cell below, update the display_images function by changing the **numOfImages** parameter to a number from 1 to 5. Click **Save**, and then click **Run**.\n",
    " \n",
    "*Hint: The display_images function sets a display grid showing NxN pet images. The default number of images is set to **?**. Change the **?** character to something greater than 1; for example, **numOfImages = 6**.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "def get_category(file):\n",
    "    m = re.search(\"\\d\", file, re.IGNORECASE)\n",
    "    if m:\n",
    "        return file[:m.start() - 1].lower().split(\"/\")[1]\n",
    "\n",
    "def display_images(file_names, numOfImages = ?):\n",
    "    indicies = random.sample(range(len(file_names)), numOfImages * numOfImages)\n",
    "    train_images = [file_names[i] for i in indicies]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=numOfImages,ncols=numOfImages, figsize=(15,15), sharex=True, sharey=True, frameon=False)\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        curr_i = train_images[i]\n",
    "        imgplot = mpimg.imread(curr_i)\n",
    "        ax.imshow(imgplot)\n",
    "        ax.text(10,20,get_category(curr_i), fontdict={\"backgroundcolor\": \"black\",\"color\": \"white\" })\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)    \n",
    "    \n",
    "    \n",
    "display_images(glob.glob('breeds/*.jpg'))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Clean and Normalize the Data\n",
    "Existing image recognition data-sets often include images of multiple dimensions, color mixed with black and white photos, maybe even line art plus photos. File names may follow multiple formats, and the subject matter within the images may be single, multiple, profile, straight-on face, back of head, surrounded by a complex background or more. \n",
    "Cleaning and normalizing the data means fixing the inconsistencies so that the machine processing can occur with minimal errors. Oftentimes data cleaning is tedious and requires significant time commitment. \n",
    "Data preprocessing techniques include:\n",
    "1.\tData cleaning − Eliminates noise and resolves inconsistencies in the data. \n",
    "2.\tData integration − Migrates data from various different sources into one coherent source, such as a data warehouse.\n",
    "3.\tData transformation – Standardizes or normalizes any form of data.\n",
    "4.\tData reduction – Reduces the size of the data by aggregating it.\n",
    "\n",
    "Another name for this effort is extract, transform, and load (ETL).\n",
    "This project required the team to normalize the file dimensions, file names and create a data layout expected by the framework. \n",
    "\n",
    "It is common for the data cleanup tasks to be pared with framework and topology selection because different topologies expect different data layouts and formats. When experimenting with different topologies it might be necessary to have several copies of the data in various formats.  Multiple copies of data-sets can take up a lot of space, so ensure you’ve got lots of storage and processing capability.\n",
    "\n",
    "![Clean and Normalize the Data](assets/part1_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "The code in the next cell performs some of the cleanup tasks. Review the code and notice that it is removing corrupt files, files with the wrong format, and files with incorrect metadata.\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "for file in glob.glob(\"breeds/*\"):\n",
    "    if not file.endswith(\".jpg\"):\n",
    "        #Not ending in .jpg\n",
    "        print(\"Deleting (.mat): \" + file)\n",
    "        os.remove(os.path.join(os.getcwd(), file))\n",
    "    else: \n",
    "        flags = cv2.IMREAD_COLOR\n",
    "        im = cv2.imread(file, flags)\n",
    "        \n",
    "        if im is None:\n",
    "            #Can't read in image\n",
    "            print(\"Deleting (None): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "        elif len(im.shape) != 3:\n",
    "            #Wrong amount of channels\n",
    "            print(\"Deleting (len != 3): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "        elif im.shape[2] != 3:\n",
    "            #Wrong amount of channels\n",
    "            print(\"Deleting (shape[2] != 3): \" + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "            continue\n",
    "            \n",
    "        with open(os.path.join(os.getcwd(), file), 'rb') as f:\n",
    "            check_chars = f.read()\n",
    "        if check_chars[-2:] != b'\\xff\\xd9':\n",
    "            #Wrong ending metadata for jpg standard\n",
    "            print('Deleting (xd9): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif check_chars[:4] != b'\\xff\\xd8\\xff\\xe0':\n",
    "            #Wrong Start Marker / JFIF Marker metadata for jpg standard\n",
    "            print('Deleting (xd8/xe0): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif check_chars[6:10] != b'JFIF':\n",
    "            #Wrong Identifier metadata for jpg standard\n",
    "            print('Deleting (xd8/xe0): ' + file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))\n",
    "        elif \"beagle_116.jpg\" in file or \"chihuahua_121.jpg\" in file:\n",
    "            #Using EXIF Data to determine this\n",
    "            print('Deleting (corrupt jpeg data): ', file)\n",
    "            os.remove(os.path.join(os.getcwd(), file))  \n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Your Data\n",
    "\n",
    "Most of the time you’re cleaning data and removing noise. Since our app needs to work with images of wet, muddy, or injured animals, or perhaps blurry images because the animal is running away in fear, we actually need to ADD noise to the data-set. \n",
    "\n",
    "We decided to add image noise by building a small program to flip, flop, blur, and extract color channels from the images in the dataset. These actions expanded our training data-set by 6x.\n",
    "\n",
    "The cell below uses a parallel method to scale the image processing tasks to all available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "\n",
    "#echo \"Start resizing to 227x227\"\n",
    "#parallel -j 200 convert {} -resize 227x227 -filter spline -unsharp 0x6+0.5+0 -background black -gravity center -extent 227x227  {} ::: *.jpg\n",
    "#echo \"Resizing done\"\n",
    "\n",
    "#mkdir flop\n",
    "#echo \"Start augmentation 1\"\n",
    "#parallel -j 200 convert {} -flop flop/{.}-flop.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 1\"\n",
    "\n",
    "#mkdir flip\n",
    "#echo \"Start augmentation 2\"\n",
    "#parallel -j 200 convert {} -transverse -rotate 90 flip/{.}-flip.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 2\"\n",
    "\n",
    "#mkdir blur\n",
    "#echo \"Start augmentation 3\"\n",
    "#parallel -j 200 convert {} -blur 0x1 blur/{.}-blur.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 3\"\n",
    "\n",
    "#mkdir red\n",
    "#echo \"Start augmentation 4\"\n",
    "#parallel -j 200 convert {} -channel R -separate red/{.}-red.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 4\"\n",
    "\n",
    "#mkdir blue\n",
    "#echo \"Start augmentation 5\"\n",
    "#parallel -j 200 convert {} -channel B -separate blue/{.}-blue.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 5\"\n",
    "\n",
    "#mkdir green\n",
    "#echo \"Start augmentation 6\"\n",
    "#parallel -j 200 convert {} -channel G -separate green/{.}-green.jpg ::: *.jpg\n",
    "#echo \"Finish augmetation 6\"\n",
    "\n",
    "#echo \"Copying augmented data to main folder\"\n",
    "#cp flop/* flip/* blur/* red/* blue/* green/* .\n",
    "\n",
    "#echo \"Augmentation done\"\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "def resize_image(file, size=224):\n",
    "    black_background = Image.new('RGB', (size, size), \"black\")\n",
    "    img = Image.open(file)\n",
    "    img.thumbnail((size,size))\n",
    "    x, y = img.size\n",
    "    black_background.paste(img, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    black_background.save(file)\n",
    "    return black_background\n",
    "  \n",
    "pool = Pool()\n",
    "for i, _ in enumerate(pool.map(resize_image, glob.glob(\"breeds/*\"))):\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r{0} out of {1} processed'.format(i+1, len(glob.glob(\"breeds/*\"))))\n",
    "        \n",
    "sys.stdout.write('\\n')\n",
    "sys.stdout.flush()\n",
    "\n",
    "display_images(glob.glob('breeds/*.jpg'))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "# Organize Data for Consumption by TensorFlow*\n",
    "\n",
    "The framework you choose for your project determines how you need to organize your data. After extensive experimentation we selected TensorFlow for this project. This section describes how to organize your data layers.\n",
    "\n",
    "We are splitting the images into training and validation sets, with 80 percent of the images targeted for training and 20 percent of the images targeted for validation.  Our data needs to be organized in a specific manner. That organization is to have each image in a folder that dictates which category it belongs to.  \n",
    "\n",
    "We'll create a train and a validation folder.  Within those folders, we'll have directories with each category name and then the respective images within their category folder.\n",
    "\n",
    "This next cell of code creates the data layout as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Organize Data for Consumption by Framework](assets/part1_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity \n",
    "In the cell below, set the **train_ratio** to **0.8** and then click **Run**.\n",
    "\n",
    "*Hint: We set the train_ratio = ? to a value between 0 and 1 to define our train and validation split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import errno\n",
    "import math\n",
    "\n",
    "def get_category(file):\n",
    "    m = re.search(\"\\d\", file, re.IGNORECASE)\n",
    "    if m:\n",
    "        return file[:m.start() - 1].lower()\n",
    "\n",
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "train_ratio = ?\n",
    "        \n",
    "file_names = os.listdir('breeds')\n",
    "category_names = [ get_category(file) for file in file_names]\n",
    "category_names = [ name for name in category_names if name is not None ]\n",
    "category_names = sorted(list(set(category_names)))\n",
    "for category in category_names:\n",
    "    make_sure_path_exists(\"breeds/train/\" + str(category))\n",
    "    make_sure_path_exists(\"breeds/validation/\" + str(category)) \n",
    "\n",
    "   \n",
    "for idx, category in enumerate(category_names):\n",
    "    category_list = []\n",
    "    for file in file_names:\n",
    "        if category.lower() in file.lower():\n",
    "            category_list.append(file)\n",
    "    \n",
    "    category_list = sorted(category_list)\n",
    "    split_ratio = math.floor(len(category_list) * train_ratio)\n",
    "    train_list = category_list[:split_ratio]\n",
    "    validation_list = category_list[split_ratio:]\n",
    "    for i, file in enumerate(train_list):\n",
    "        os.rename(\"breeds/\" + file, \"breeds/train/\" + str(category) + \"/\" + file)\n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r>> Moving train image %d to category folder %s' % (i+1, category))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()        \n",
    "        \n",
    "    for i, file in enumerate(validation_list):\n",
    "        os.rename(\"breeds/\" + file, \"breeds/validation/\" + str(category) + \"/\" + file)\n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r>> Moving validation image %d to category folder %s' % (i+1, category))\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()      \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    " \n",
    "# Confirm Folder Structure is Correct\n",
    "\n",
    "We have a sorted folder, 37 breeds folders, and pictures of those breeds within their respective folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confirm Folder Structure is Correct](assets/part1_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity \n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"breeds\"):\n",
    "    level = root.replace(os.getcwd(), '').count(os.sep)\n",
    "    print('{0}{1}/'.format('    ' * level, os.path.basename(root)))\n",
    "    for f in files[:5]:\n",
    "        print('{0}{1}'.format('    ' * (level + 1), f))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Data for Ingestion\n",
    "\n",
    "### Data Input/Output\n",
    "A TFRecords file represents a sequence of (binary) strings. The format is not random access, so it is suitable for streaming large amounts of data but not suitable if fast sharding or other non-sequential access is desired. See Data IO (Python Functions), https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details\n",
    "\n",
    "### Standard TensorFlow* Format\n",
    "Another approach is to convert whatever data you have into a supported format. This approach makes it easier to mix and match data-sets and network architectures. The recommended format for TensorFlow is a TFRecords file containing tf.train.Example protocol buffers (which contain Features as a field). You write a little program that gets your data, stuffs it in an Example protocol buffer, serializes the protocol buffer to a string, and then writes the string to a TFRecords file using the tf.python_io.TFRecordWriter. See https://www.tensorflow.org/versions/r1.0/programmers_guide/reading_data#file_formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimize Data for Ingestion](assets/part1_6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "When creating a TFRecord file you can split the dataset into shards.  This can be especially beneficial if you have a particularly large dataset and don't want to end up with a single 1+GB file.  \n",
    "\n",
    "Below, we're creating shards of data based on the files into the number passed in **\\_NUM\\_SHARDS**.\n",
    "\n",
    "In the cell below, set **\\_NUM\\_SHARDS** to a value between **1** and **5** and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "_NUM_SHARDS = ?\n",
    "_SHARD_NAME = \"breeds\"\n",
    "LABELS_FILENAME = 'labels.txt'\n",
    "\n",
    "class ImageReader(object):\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_jpeg(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_jpeg(self, sess, image_data):\n",
    "        image = sess.run(self._decode_jpeg,\n",
    "                         feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "\n",
    "def int64_feature(values):\n",
    "    if not isinstance(values, (tuple, list)):\n",
    "        values = [values]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "\n",
    "def bytes_feature(values):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "    \n",
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/encoded': bytes_feature(image_data),\n",
    "      'image/format': bytes_feature(image_format),\n",
    "      'image/class/label': int64_feature(class_id),\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width),\n",
    "    }))\n",
    "\n",
    "def write_label_file(labels_to_class_names, dataset_dir,\n",
    "                     filename=LABELS_FILENAME):\n",
    "    labels_filename = os.path.join(dataset_dir, filename)\n",
    "    with tf.gfile.Open(labels_filename, 'w') as f:\n",
    "        for label in labels_to_class_names:\n",
    "            class_name = labels_to_class_names[label]\n",
    "            f.write('%d:%s\\n' % (label, class_name))\n",
    "\n",
    "\n",
    "def _get_filenames_and_classes(dataset_dir, sorted_dir):\n",
    "    breeds_root = os.path.join(dataset_dir, sorted_dir)\n",
    "    directories = []\n",
    "    class_names = []\n",
    "    for filename in os.listdir(breeds_root):\n",
    "        path = os.path.join(breeds_root, filename)\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "            class_names.append(filename)\n",
    "\n",
    "    photo_filenames = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            path = os.path.join(directory, filename)\n",
    "            photo_filenames.append(path)\n",
    "\n",
    "    return photo_filenames, sorted(class_names)\n",
    "\n",
    "\n",
    "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "    output_filename = _SHARD_NAME + '_%s_%05d-of-%05d.tfrecord' % (\n",
    "      split_name, shard_id, _NUM_SHARDS)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "\n",
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "    assert split_name in ['train', 'validation']\n",
    "\n",
    "    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = ImageReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "\n",
    "            for shard_id in range(_NUM_SHARDS):\n",
    "                output_filename = _get_dataset_filename(\n",
    "                    dataset_dir, split_name, shard_id)\n",
    "\n",
    "                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                    start_ndx = shard_id * num_per_shard\n",
    "                    end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "                    for i in range(start_ndx, end_ndx):\n",
    "                        sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                            i+1, len(filenames), shard_id))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Read the filename:\n",
    "                        image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "                        height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "                        class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "                        class_id = class_names_to_ids[class_name]\n",
    "\n",
    "                        example = image_to_tfexample(\n",
    "                            image_data, b'jpg', height, width, class_id)\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _dataset_exists(dataset_dir):\n",
    "    for split_name in ['train', 'validation']:\n",
    "        for shard_id in range(_NUM_SHARDS):\n",
    "            output_filename = _get_dataset_filename(\n",
    "              dataset_dir, split_name, shard_id)\n",
    "            if not tf.gfile.Exists(output_filename):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "TensorFlow requires separate data sets for training and validation and that the data be stored in two separate records. Why separate image sets for training and validation? To prevent *overfitting*, which occurs when you train and test on the same images. You train on a set, then test on a new/different set to validate that the machine is truly learning to recognize the images. \n",
    "\n",
    "Our records will contain the words **train** and **validation** in their path to distinguish between the two. We used the industry standard ratio of 80 percent train and 20 percent test/validation to split the data-set.\n",
    "\n",
    "In the cell below, set the two function calls to **\\_convert\\_dataset\\_** first parameter to **\"train\"** and **\"validation\"** and then click **Run**.\n",
    "\n",
    "*Hint: Look at the filenames being passed into the **\\_convert\\_dataset\\_** function and make sure you are matching that with the correct label you are replacing into the **?????**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def run(dataset_dir):\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "    if _dataset_exists(dataset_dir):\n",
    "        print('Dataset files already exist. Exiting without re-creating them.')\n",
    "        return\n",
    "\n",
    "    train_photo_filenames, class_names = _get_filenames_and_classes(dataset_dir, \"train\")\n",
    "    validation_photo_filenames, class_names = _get_filenames_and_classes(dataset_dir, \"validation\")\n",
    "    class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "    \n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset(?????, train_photo_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "    _convert_dataset(????????, validation_photo_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "    write_label_file(labels_to_class_names, dataset_dir)\n",
    "\n",
    "    print('\\nFinished converting the Breeds dataset!')\n",
    "\n",
    "run('breeds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### After All of This Data Wrangling We Can Actually Begin the Training Process\n",
    "\n",
    "When we started this project, we always had an edge device in mind as our ultimate deployment platform. To that end we always considered three things when selecting our topology or network: time to train, size, and inference speed. \n",
    "\n",
    "**Time to Train:** Depending on the number of layers and computation required, a network can take a significantly shorter or longer time to train. Computation time and programmer time are costly resources, so we wanted short training times.  \n",
    "\n",
    "**Size:** Since we're targeting an edge device and a an Intel® Movidius™ Neural Compute Stick stick we must consider the size of the network that is allowed in memory as well as supported networks.\n",
    "\n",
    "**Inference Speed:** Typically the deeper and larger the network, the slower the inference speed. In our use case we are working with a live video stream; we want at least 10 frames per second on inference.\n",
    "\n",
    "At this point we're going to continue with the TensorFlow framework plus the GoogLeNet Inception* v1 topology/network since we're currently working on a simpler dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GoogLeNet](assets/googlenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training CatVsDog with TensorFlow and GoogLeNet Inception* v1 on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "Understand the stages of preparing for training using the TensorFlow framework and an GoogLeNet Inception v1 topology. You will initiate training and view a completed graph, and learn about the relationship between accuracy and loss.\n",
    "\n",
    "# Activities \n",
    "**In this section of the training you will**\n",
    "- Download pretrained model\n",
    "- Clone TensorFlow/models Github* repo\n",
    "- Modify/add files within repo to add our dataset\n",
    "- Initiate training and review live training logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Models\n",
    "\"Neural nets work best when they have many parameters, making them powerful function approximators. However, this means they must be trained on very large datasets. Because training models from scratch can be a very computationally intensive process requiring days or even weeks, we are using a pre-trained models provided by Google. This CNNs have been trained on the ILSVRC-2012-CLS image classification dataset.\" From https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Download pre-trained model](assets/part2_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n",
    "!tar xf inception_v1_2016_08_28.tar.gz\n",
    "!rm -rf checkpoints\n",
    "!mkdir checkpoints\n",
    "!mv inception_v1.ckpt checkpoints\n",
    "!rm inception_v1_2016_08_28.tar.gz\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Models\n",
    "\n",
    "The TensorFlow team provides nice wrappers around a lot of functionality that needs to be done when training using TensorFlow.  Below, we're going to pull in one of these repos directly so that we have access to those wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clone TensorFlow Models Repo](assets/part2_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/tensorflow/models\n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the Slim Datasets\n",
    "\n",
    "To use the wrappers we're going to have to modify and add some existing code to the repo.  Below we're overwriting the **dataset_factory.py** file with a slightly modified version that knows about our breeds dataset and an additional Python* import statement.  We're also copying over **breeds.py** since this contains information specific to our dataset that will be utilized by the **dataset_factory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modify Repo Scripts](assets/part2_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp breeds.py models/research/slim/datasets/breeds.py\n",
    "!cp dataset_factory_modified.py models/research/slim/datasets/dataset_factory.py\n",
    "!cp train_image_classifier_modified.py models/research/slim/train_image_classifier.py\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training\n",
    "\n",
    "Let’s start training with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPUs, which includes Intel® Xeon Phi™ processors, achieve optimal performance when TensorFlow is built from source with all of the instructions supported by the target CPU.\n",
    "\n",
    "Beyond using the latest instruction sets, Intel has added support for the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) to TensorFlow. While the name is not completely accurate, these optimizations are often simply referred to as MKL or *TensorFlow with MKL*. TensorFlow with Intel MKL-DNN contains details on the Intel® MKL optimizations.\n",
    "\n",
    "The two configurations listed below are used to optimize CPU performance by adjusting the thread pools.\n",
    "\n",
    "- **intra_op_parallelism_threads**: Nodes that can use multiple threads to parallelize their execution will schedule the individual pieces into this pool.\n",
    "- **inter_op_parallelism_threads**: All ready nodes are scheduled in this pool.\n",
    "\n",
    "These configurations are set via the tf.ConfigProto and passed to tf.Session in the config attribute as shown in the snippet below. For both configuration options, if they are unset or set to zero, will default to the number of logical CPU cores. Testing has shown that the default is effective for systems ranging from one CPU with 4 cores to multiple CPUs with 70+ combined logical cores. A common alternative optimization is to set the number of threads in both pools equal to the number of physical cores rather than logical cores.\n",
    "\n",
    "Intel MKL uses the following environment variables to tune performance:\n",
    "\n",
    "**KMP_BLOCKTIME** - Sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping.\n",
    "\n",
    "**KMP_AFFINITY** - Enables the runtime library to bind threads to physical processing units.\n",
    "\n",
    "**KMP_SETTINGS** - Enables (true) or disables (false) the printing of OpenMP* runtime library environment variables during program execution.\n",
    "\n",
    "**OMP_NUM_THREADS** - Specifies the number of threads to use.\n",
    "\n",
    "See *Optimizing for CPU*, https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu.\n",
    "\n",
    "**Best Settings for Intel® Xeon Processor - 5th Generation  (2 Socket -- 44 Cores)**\n",
    "![Tensorflow Optimization](assets/tf_optimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimize Performance for CPU](assets/part2_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "In the cell below, update **OMP_NUM_THREADS** to **\"12\"**, **KMP_BLOCKTIME** to **\"1\"**, and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"] = ?\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = ??\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Model from an Existing Checkpoint\n",
    "\n",
    "\"Rather than training from scratch, we'll often want to start from a pre-trained model and fine-tune it. To indicate a checkpoint from which to fine-tune, we'll call training with the --checkpoint_path flag and assign it an absolute path to a checkpoint file.\n",
    "\n",
    "When fine-tuning a model, we need to be careful about restoring checkpoint weights. In particular, when we fine-tune a model on a new task with a different number of output labels, we wont be able restore the final logits (classifier) layer. For this, we'll use the --checkpoint_exclude_scopes flag. This flag hinders certain variables from being loaded. When fine-tuning on a classification task using a different number of classes than the trained model, the new model will have a final 'logits' layer whose dimensions differ from the pre-trained model. For example, if fine-tuning an ImageNet-trained model on Flowers, the pre-trained logits layer will have dimensions [2048 x 1001] but our new logits layer will have dimensions [2048 x 5]. Consequently, this flag indicates to TF-Slim to avoid loading these weights from the checkpoint.\n",
    "\n",
    "Keep in mind that warm-starting from a checkpoint affects the model's weights only during the initialization of the model. Once a model has started training, a new checkpoint will be created in --train_dir. If the fine-tuning training is stopped and restarted, this new checkpoint will be the one from which weights are restored and not the --checkpoint_path. Consequently, the flags --checkpoint_path and --checkpoint_exclude_scopes are only used during the 0-th global step (model initialization). Typically for fine-tuning one only want train a sub-set of layers, so the flag --trainable_scopes allows to specify which subsets of layers should trained, the rest would remain frozen.\" See https://github.com/tensorflow/models/tree/master/research/slim#fine-tuning-a-model-from-an-existing-checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tune a Model](assets/part2_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "In the cell below, update the **max_number_of_steps** parameter to a number between **500** and **1500**, the **intra_op** parameter to the number **12** and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf train_dir\n",
    "!mkdir train_dir\n",
    "\n",
    "!python models/research/slim/train_image_classifier.py \\\n",
    "    --train_dir=train_dir \\\n",
    "    --dataset_name=breeds \\\n",
    "    --dataset_split_name=train \\\n",
    "    --clone_on_cpu=true \\\n",
    "    --dataset_dir=breeds \\\n",
    "    --model_name=inception_v1 \\\n",
    "    --checkpoint_path=checkpoints/inception_v1.ckpt \\\n",
    "    --checkpoint_exclude_scopes=InceptionV1/Logits \\\n",
    "    --trainable_scopes=InceptionV1/Logits \\\n",
    "    --max_number_of_steps=???? \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=32 \\\n",
    "    --save_interval_secs=60 \\\n",
    "    --save_summaries_secs=60 \\\n",
    "    --inter_op=2 \\\n",
    "    --intra_op=??\n",
    "\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Evaluate, Freeze and Test Your Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Latest Training Checkpoint\n",
    "\n",
    "Earlier we created a TFRecord file with our validation images.  Below, we'll be using our validation set to determine our accuracy by running the eval_image_classifier script.  It will give us the Accuracy and Recall for Top 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluate Your Checkpoint](assets/part3_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf eval_dir\n",
    "!mkdir eval_dir\n",
    "!python models/research/slim/eval_image_classifier.py \\\n",
    "    --checkpoint_path=$(ls -t train_dir/model.ckpt* | head -1 | rev | cut -d '.' -f2- | rev) \\\n",
    "    --eval_dir=eval_dir \\\n",
    "    --dataset_dir=breeds \\\n",
    "    --dataset_name=breeds \\\n",
    "    --dataset_split_name=validation \\\n",
    "    --model_name=inception_v1\n",
    "\n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Your Inference Graph of Inception v1\n",
    "\n",
    "We want to export our inference graph of Inception v1 so we can use it later to create a frozen graph (.pb) file.  Below, we'll run the export_inference_graph script that will take the inceptionv1 model and our dataset to create a .pb file.  Passing in our dataset is important since it will make sure to create a final layer of 37 categories rather than the 1000 from ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Export Inference Graph](assets/part3_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/research/slim/export_inference_graph.py \\\n",
    "    --alsologtostderr \\\n",
    "    --model_name=inception_v1 \\\n",
    "    --image_size=224 \\\n",
    "    --batch_size=1 \\\n",
    "    --output_file=train_dir/inception_v1_inf_graph.pb \\\n",
    "    --dataset_name=breeds\n",
    "    \n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the Main TensorfFow Repo\n",
    "\n",
    "We're cloning the main TensorFlow/TensorFlow repository since it contains the script to create a frozen graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clone TensorFlow Repo](assets/part3_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/tensorflow/tensorflow.git\n",
    "    \n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze Your Graph\n",
    "\n",
    "Freezing your graph will take the inference graph definition we created above and the latest checkpoint file that was created during training.  It will merge these two into a single file for a convenient way to have the graph definition and weights for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Freeze Your Graph](assets/part3_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/python/tools/freeze_graph.py \\\n",
    "    --clear_devices=true \\\n",
    "    --input_graph=train_dir/inception_v1_inf_graph.pb \\\n",
    "    --input_checkpoint=$(ls -t train_dir/model.ckpt* | head -1 | rev | cut -d '.' -f2- | rev) \\\n",
    "    --input_binary=true \\\n",
    "    --output_graph=train_dir/frozen_inception_v1.pb \\\n",
    "    --output_node_names=InceptionV1/Logits/Predictions/Reshape_1\n",
    "    \n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at a Sample Image\n",
    "\n",
    "We're going to use this image to run through the network and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Display a Sample Image](assets/part3_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open('breeds/train/maine_coon/Maine_Coon_100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on an Image\n",
    "\n",
    "We can use the newly created frozen graph file to test a sample image.  We're using the label_image script that takes an image, frozen graph, labels.txt files, and displays the top five probabilities for the given image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inference on Image](assets/part3_6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/label_image/label_image.py \\\n",
    "    --image=breeds/train/maine_coon/Maine_Coon_100.jpg \\\n",
    "    --input_layer=input \\\n",
    "    --input_height=224 \\\n",
    "    --input_width=224 \\\n",
    "    --output_layer=InceptionV1/Logits/Predictions/Reshape_1 \\\n",
    "    --graph=train_dir/frozen_inception_v1.pb \\\n",
    "    --labels=breeds/labels.txt\n",
    "    \n",
    "print(\"Done.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting your dataset\n",
    "- Sorting your dataset\n",
    "- Generating TFRecord files\n",
    "- Learning about fine-tuning and checkpoints\n",
    "- Train your dataset with fine-tune checkpoint\n",
    "- Evaluating your training\n",
    "- Creating a frozen graph\n",
    "- Using a frozen graph to test image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Additional Fine Tuning (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning the Entire Network\n",
    "\n",
    "We previously fine tuned only the final layer of the network.  Now we're going to allow for all of the layers in the network to be trained but we're going to use a much lower learning rate.  This will let the network narrow in and tune the remaining weights we didn't tune from the ImageNet checkpoint.  We'll want to make sure not to train too much though, or we might start to overfit, so we'll limit the steps to about 500-1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "In the cell below, update the **max_number_of_steps** parameter to a number between **500** and **1500**, the **learning_rate** to **0.0001** and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/research/slim/train_image_classifier.py \\\n",
    "    --train_dir=train_dir/all \\\n",
    "    --dataset_name=breeds \\\n",
    "    --dataset_split_name=train \\\n",
    "    --clone_on_cpu=true \\\n",
    "    --dataset_dir=breeds \\\n",
    "    --model_name=inception_v1 \\\n",
    "    --checkpoint_path=train_dir \\\n",
    "    --max_number_of_steps=???? \\\n",
    "    --learning_rate=???? \\\n",
    "    --learning_rate_decay_type=fixed \\\n",
    "    --batch_size=32 \\\n",
    "    --save_interval_secs=60 \\\n",
    "    --save_summaries_secs=60 \\\n",
    "    --inter_op=2 \\\n",
    "    --intra_op=12\n",
    "\n",
    "!echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/research/slim/eval_image_classifier.py \\\n",
    "    --checkpoint_path=$(ls -t train_dir/all/model.ckpt* | head -1 | rev | cut -d '.' -f2- | rev) \\\n",
    "    --eval_dir=eval_dir/all \\\n",
    "    --dataset_dir=breeds \\\n",
    "    --dataset_name=breeds \\\n",
    "    --dataset_split_name=validation \\\n",
    "    --model_name=inception_v1\n",
    "\n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/python/tools/freeze_graph.py \\\n",
    "    --clear_devices=true \\\n",
    "    --input_graph=train_dir/inception_v1_inf_graph.pb \\\n",
    "    --input_checkpoint=$(ls -t train_dir/all/model.ckpt* | head -1 | rev | cut -d '.' -f2- | rev) \\\n",
    "    --input_binary=true \\\n",
    "    --output_graph=train_dir/all/frozen_inception_v1.pb \\\n",
    "    --output_node_names=InceptionV1/Logits/Predictions/Reshape_1\n",
    "    \n",
    "!echo \"Done.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open('breeds/train/maine_coon/Maine_Coon_100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/label_image/label_image.py \\\n",
    "    --image=breeds/train/maine_coon/Maine_Coon_100.jpg \\\n",
    "    --input_layer=input \\\n",
    "    --input_height=224 \\\n",
    "    --input_width=224 \\\n",
    "    --output_layer=InceptionV1/Logits/Predictions/Reshape_1 \\\n",
    "    --graph=train_dir/all/frozen_inception_v1.pb \\\n",
    "    --labels=breeds/labels.txt\n",
    "    \n",
    "print(\"Done.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow* Optimizations on Modern Intel® Architecture, https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture\n",
    "\n",
    "Intel Optimized TensorFlow Wheel Now Available, https://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available\n",
    "\n",
    "Build and Install TensorFlow* on Intel® Architecture, https://software.intel.com/en-us/articles/build-and-install-tensorflow-on-intel-architecture\n",
    "\n",
    "TensorFlow, https://www.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manufacturing Package Fault Detection Using Deep Learning, https://software.intel.com/en-us/articles/manufacturing-package-fault-detection-using-deep-learning\n",
    "\n",
    "Automatic Defect Inspection Using Deep Learning for Solar Farm, https://software.intel.com/en-us/articles/automatic-defect-inspection-using-deep-learning-for-solar-farm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notices**\n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.\n",
    "\n",
    "Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.\n",
    "\n",
    "This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest forecast, schedule, specifications and roadmaps.\n",
    "\n",
    "The products and services described may contain defects or errors known as errata which may cause deviations from published specifications. Current characterized errata are available on request.\n",
    "\n",
    "Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting www.intel.com/design/literature.htm.\n",
    "\n",
    "This sample source code is released under the Intel Sample Source Code License Agreement.\n",
    "\n",
    "Intel, the Intel logo, Intel Xeon Phi, Movidius, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. \n",
    "\n",
    "*Other names and brands may be claimed as the property of others.\n",
    "\n",
    "© 2018 Intel Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
